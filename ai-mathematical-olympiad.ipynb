{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1869bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:52:42.657765Z",
     "iopub.status.busy": "2025-11-23T06:52:42.657427Z",
     "iopub.status.idle": "2025-11-23T06:52:42.665772Z",
     "shell.execute_reply": "2025-11-23T06:52:42.664854Z"
    },
    "papermill": {
     "duration": 0.014896,
     "end_time": "2025-11-23T06:52:42.667657",
     "exception": false,
     "start_time": "2025-11-23T06:52:42.652761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af95d9d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:52:42.675897Z",
     "iopub.status.busy": "2025-11-23T06:52:42.675202Z",
     "iopub.status.idle": "2025-11-23T06:53:06.371412Z",
     "shell.execute_reply": "2025-11-23T06:53:06.370418Z"
    },
    "papermill": {
     "duration": 23.702902,
     "end_time": "2025-11-23T06:53:06.373359",
     "exception": false,
     "start_time": "2025-11-23T06:52:42.670457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Local Test Mode...\n",
      "üß™ Testing basic code generation...\n",
      "‚è≥ Loading NuminaMath-7B-TIR...\n",
      "üîç Detected 0 GPU(s)\n",
      "‚ùå No GPU detected! This model cannot run on CPU within time limits.\n",
      "‚ùå Skipping test because model failed to load.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import sympy\n",
    "import itertools\n",
    "import collections\n",
    "import fractions\n",
    "import gc\n",
    "import signal\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ==========================================\n",
    "# 0. MEMORY OPTIMIZATION\n",
    "# ==========================================\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "MODEL_PATH = \"/kaggle/input/math_numina_7b_tir/pytorch/default/1/NuminaMath-7B-TIR\"\n",
    "\n",
    "class SpecialistAIMO3Solver:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.is_loaded = False\n",
    "        self.problem_cache = {}\n",
    "\n",
    "    def load(self):\n",
    "        if self.is_loaded: return\n",
    "        \n",
    "        print(f\"‚è≥ Loading NuminaMath-7B-TIR...\")\n",
    "        \n",
    "        if not os.path.exists(MODEL_PATH):\n",
    "            print(f\"‚ùå Model not found at {MODEL_PATH}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "            \n",
    "            # --- DYNAMIC HARDWARE DETECTION ---\n",
    "            # Check how many GPUs are actually available\n",
    "            num_gpus = torch.cuda.device_count()\n",
    "            print(f\"üîç Detected {num_gpus} GPU(s)\")\n",
    "            \n",
    "            if num_gpus == 0:\n",
    "                print(\"‚ùå No GPU detected! This model cannot run on CPU within time limits.\")\n",
    "                return\n",
    "                \n",
    "            # Check VRAM of the first GPU\n",
    "            vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            print(f\"üîç Primary GPU VRAM: {vram_gb:.1f} GB\")\n",
    "            \n",
    "            if vram_gb > 24:\n",
    "                # Case A: H100 (80GB), A100 (40GB), L4 (24GB) -> Single Device Load\n",
    "                print(\"üöÄ High-VRAM GPU detected! Loading full model on GPU 0...\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    MODEL_PATH,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    local_files_only=True,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "            elif num_gpus > 1:\n",
    "                # Case B: 2x T4 (15GB each) -> Split across devices\n",
    "                print(\"‚ö†Ô∏è Dual T4 GPUs detected. Splitting model...\")\n",
    "                max_mem = {0: \"11GiB\", 1: \"11GiB\", \"cpu\": \"30GiB\"}\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    MODEL_PATH,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    max_memory=max_mem,\n",
    "                    local_files_only=True,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "            else:\n",
    "                # Case C: Single T4 (15GB) -> Very tight fit, might offload to CPU\n",
    "                print(\"‚ö†Ô∏è Single Small GPU detected. Model might offload to CPU (Slow).\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    MODEL_PATH,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    local_files_only=True,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "\n",
    "            print(\"‚úÖ Model Loaded (Float16)!\")\n",
    "            self.is_loaded = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Load Failed: {e}\")\n",
    "            return\n",
    "\n",
    "    def identify_topic(self, problem_text):\n",
    "        \"\"\"Enhanced topic classification with pattern matching\"\"\"\n",
    "        problem_lower = problem_text.lower()\n",
    "        \n",
    "        # Multi-keyword scoring\n",
    "        topic_scores = {\n",
    "            \"geometry\": 0, \"number_theory\": 0, \n",
    "            \"combinatorics\": 0, \"algebra\": 0\n",
    "        }\n",
    "        \n",
    "        # Geometry patterns\n",
    "        geo_patterns = ['triangle', 'circle', 'angle', 'perimeter', 'area', \n",
    "                        'acute-angled', 'right-angled', 'congruent', 'similar',\n",
    "                        'bisector', 'circumcircle', 'radius', 'diameter']\n",
    "        for pattern in geo_patterns:\n",
    "            if pattern in problem_lower:\n",
    "                topic_scores[\"geometry\"] += 2\n",
    "        \n",
    "        # Number theory patterns  \n",
    "        nt_patterns = ['mod', 'prime', 'divisible', 'gcd', 'lcm', 'remainder', \n",
    "                      'modulo', 'coprime', 'factor', 'multiple', 'integer solution']\n",
    "        for pattern in nt_patterns:\n",
    "            if pattern in problem_lower:\n",
    "                topic_scores[\"number_theory\"] += 2\n",
    "                \n",
    "        # Combinatorics patterns\n",
    "        comb_patterns = ['combination', 'permutation', 'probability', 'choose', \n",
    "                        'arrangement', 'tournament', 'runner', 'race', 'count']\n",
    "        for pattern in comb_patterns:\n",
    "            if pattern in problem_lower:\n",
    "                topic_scores[\"combinatorics\"] += 2\n",
    "        \n",
    "        # Algebra patterns\n",
    "        algebra_patterns = ['function', 'polynomial', 'equation', 'solve for', 'root',\n",
    "                           'coefficient', 'variable', 'expression']\n",
    "        for pattern in algebra_patterns:\n",
    "            if pattern in problem_lower:\n",
    "                topic_scores[\"algebra\"] += 2\n",
    "    \n",
    "        # Return highest scored topic\n",
    "        best_topic = max(topic_scores, key=topic_scores.get)\n",
    "        return best_topic if topic_scores[best_topic] > 0 else \"general\"\n",
    "\n",
    "    def get_specialist_prompt(self, problem_text, topic):\n",
    "        \"\"\"Domain-Specific Prompts to Force Correct Reasoning\"\"\"\n",
    "        \n",
    "        base_prompt = f\"Problem: {problem_text}\\n\\n\"\n",
    "        \n",
    "        if topic == \"geometry\":\n",
    "            return base_prompt + \"\"\"Solve this GEOMETRY problem using coordinate geometry or sympy.\n",
    "CRITICAL: Your code MUST print the final integer answer using print().\n",
    "Example: print(42)\n",
    "\n",
    "Python code:\n",
    "```python\n",
    "import sympy as sp\n",
    "import math\n",
    "# Use coordinate geometry or sympy geometric objects\n",
    "\"\"\"\n",
    "        \n",
    "        elif topic == \"number_theory\":\n",
    "            return base_prompt + \"\"\"Solve this NUMBER THEORY problem using modular arithmetic.\n",
    "CRITICAL: Your code MUST print the final integer answer using print().\n",
    "Example: print(42)\n",
    "\n",
    "Python code:\n",
    "```python\n",
    "import math\n",
    "import sympy\n",
    "# Use modular arithmetic and number theory functions\n",
    "\"\"\"\n",
    "        \n",
    "        elif topic == \"combinatorics\":\n",
    "            return base_prompt + \"\"\"Solve this COMBINATORICS problem using itertools or counting principles.\n",
    "Use efficient combinatorial algorithms.\n",
    "Key techniques: combinations, permutations, inclusion-exclusion.\n",
    "IMPORTANT: Use itertools for combinatorial operations.\n",
    "\n",
    "Python code:\n",
    "```python\n",
    "import itertools\n",
    "import math\n",
    "# Use combinatorial functions and counting\n",
    "\"\"\"\n",
    "        \n",
    "        elif topic == \"algebra\":\n",
    "            return base_prompt + \"\"\"Solve this ALGEBRA problem using symbolic mathematics.\n",
    "Use sympy for equation solving and simplification.\n",
    "Key techniques: equation solving, polynomial manipulation.\n",
    "IMPORTANT: Use sympy for algebraic manipulations.\n",
    "\n",
    "Python code:\n",
    "```python\n",
    "import sympy as sp\n",
    "# Use symbolic algebra and equation solving\n",
    "\"\"\"\n",
    "        \n",
    "        else:  # general\n",
    "            return base_prompt + \"\"\"Solve this mathematical problem using efficient Python code.\n",
    "Use appropriate mathematical libraries and avoid brute force.\n",
    "Print the final integer answer.\n",
    "\n",
    "Python code:\n",
    "```python\n",
    "import math\n",
    "import sympy\n",
    "# Use appropriate mathematical approach\n",
    "\"\"\"\n",
    "\n",
    "    def run_python_code(self, code):\n",
    "        \"\"\"REAL Python Execution with Enhanced Auto-Print\"\"\"\n",
    "        sys.set_int_max_str_digits(0)\n",
    "        local_scope = {\n",
    "            \"math\": math, \"np\": np, \"numpy\": np, \"sympy\": sympy, \"itertools\": itertools,\n",
    "            \"collections\": collections, \"fractions\": fractions, \"print\": print,\n",
    "            \"sys\": sys, \"Counter\": Counter, \"sp\": sympy\n",
    "        }\n",
    "        \n",
    "        old_stdout = sys.stdout\n",
    "        redirected_output = sys.stdout = StringIO()\n",
    "        \n",
    "        def timeout_handler(signum, frame):\n",
    "            raise TimeoutError(\"Execution Timed Out\")\n",
    "        \n",
    "        # 10-second timeout for complex problems\n",
    "        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "        signal.alarm(10)\n",
    "        \n",
    "        try:\n",
    "            # Clean the code\n",
    "            code = re.sub(r\"```python|```\", \"\", code).strip()\n",
    "            \n",
    "            # FIX: Add print statement if the last line looks like a result\n",
    "            lines = [line.strip() for line in code.split('\\n') if line.strip() and not line.startswith('#')]\n",
    "            \n",
    "            if lines and 'print(' not in code:\n",
    "                last_line = lines[-1]\n",
    "                \n",
    "                # Case 1: Simple variable assignment (result = ...)\n",
    "                if '=' in last_line and not last_line.startswith(('def ', 'class ', 'import ', 'from ')):\n",
    "                    var_name = last_line.split('=')[0].strip()\n",
    "                    if re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', var_name):\n",
    "                        code += f\"\\nprint({var_name})\"\n",
    "                \n",
    "                # Case 2: Standalone variable or function call\n",
    "                elif (re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', last_line) or \n",
    "                      re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*\\(', last_line)):\n",
    "                    code += f\"\\nprint({last_line})\"\n",
    "                \n",
    "                # Case 3: Mathematical expression\n",
    "                elif re.match(r'^[\\d+\\-*/(). ]+$', last_line) and len(last_line) < 50:\n",
    "                    code += f\"\\nprint({last_line})\"\n",
    "            \n",
    "            exec(code, local_scope)\n",
    "            signal.alarm(0)\n",
    "            sys.stdout = old_stdout\n",
    "            return redirected_output.getvalue().strip(), \"SUCCESS\"\n",
    "        except TimeoutError:\n",
    "            signal.alarm(0)\n",
    "            sys.stdout = old_stdout\n",
    "            return \"Error: Code took too long (>10s).\", \"TIMEOUT\"\n",
    "        except Exception as e:\n",
    "            signal.alarm(0)\n",
    "            sys.stdout = old_stdout\n",
    "            return f\"Runtime Error: {e}\", \"ERROR\"\n",
    "\n",
    "    def extract_answer(self, text):\n",
    "        \"\"\"Enhanced answer extraction with multiple patterns\"\"\"\n",
    "        try:\n",
    "            # Look for boxed answers \\boxed{123}\n",
    "            boxed_match = re.search(r'\\\\boxed\\{(\\d+)\\}', text)\n",
    "            if boxed_match:\n",
    "                return int(boxed_match.group(1))\n",
    "                \n",
    "            # Look for final answer patterns\n",
    "            final_patterns = [\n",
    "                r'final answer[:\\s]*(\\d+)',\n",
    "                r'answer[:\\s]*(\\d+)',\n",
    "                r'result[:\\s]*(\\d+)',\n",
    "                r'solution[:\\s]*(\\d+)'\n",
    "            ]\n",
    "            for pattern in final_patterns:\n",
    "                match = re.search(pattern, text.lower())\n",
    "                if match:\n",
    "                    return int(match.group(1))\n",
    "            \n",
    "            # Extract all numbers and take the most plausible one\n",
    "            numbers = re.findall(r'\\b\\d{1,5}\\b', text)\n",
    "            if numbers:\n",
    "                # Prefer numbers that appear in final positions\n",
    "                candidates = []\n",
    "                for i, num in enumerate(numbers):\n",
    "                    num_int = int(num)\n",
    "                    if 0 <= num_int <= 99999:\n",
    "                        # Weight by position (later numbers are more likely answers)\n",
    "                        position_weight = i / len(numbers)\n",
    "                        candidates.append((num_int, position_weight))\n",
    "                \n",
    "                if candidates:\n",
    "                    # Return the number with highest position weight\n",
    "                    return max(candidates, key=lambda x: x[1])[0]\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Answer extraction error: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def verify_solution(self, problem_text, generated_code, answer):\n",
    "        \"\"\"Basic sanity checks on generated solutions\"\"\"\n",
    "        if answer is None:\n",
    "            return False\n",
    "            \n",
    "        # Check if answer is within reasonable bounds\n",
    "        if not (0 <= answer <= 99999):\n",
    "            return False\n",
    "            \n",
    "        # Check if code actually computes something mathematical\n",
    "        math_keywords = ['math.', 'sympy.', 'np.', 'import', 'def ', 'calculate', 'compute']\n",
    "        if not any(keyword in generated_code for keyword in math_keywords):\n",
    "            print(\"‚ö†Ô∏è Generated code doesn't look mathematical\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def solve(self, problem_text, max_retries=1):\n",
    "        if not self.is_loaded: \n",
    "            self.load()\n",
    "        if not self.is_loaded: \n",
    "            return 0\n",
    "        \n",
    "        problem_hash = hash(problem_text)\n",
    "        if problem_hash in self.problem_cache:\n",
    "            return self.problem_cache[problem_hash]\n",
    "        \n",
    "        # STEP 1: Classify Problem Domain\n",
    "        topic = self.identify_topic(problem_text)\n",
    "        print(f\"üß† [{topic.upper()}] Solving: {problem_text[:50]}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # STEP 2: Get Domain-Specific Prompt\n",
    "        prompt_content = self.get_specialist_prompt(problem_text, topic)\n",
    "        \n",
    "        # --- MANUAL CHAT TEMPLATE ---\n",
    "        full_prompt = f\"<|user|>\\n{prompt_content}\\nPlease write Python code in ```python ... ``` blocks.\\n<|assistant|>\\n\"\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            try:\n",
    "                # Use the manual chat template prompt\n",
    "                inputs = self.tokenizer(\n",
    "                    full_prompt, \n",
    "                    return_tensors=\"pt\", \n",
    "                    max_length=1024,\n",
    "                    truncation=True,\n",
    "                    padding=True\n",
    "                ).to(self.model.device)\n",
    "                \n",
    "                print(f\"    Generating code (attempt {attempt+1})...\")\n",
    "                generation_start = time.time()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    generated_ids = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=512,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        repetition_penalty=1.1\n",
    "                    )\n",
    "                \n",
    "                gen_time = time.time() - generation_start\n",
    "                print(f\"    Generation took {gen_time:.1f}s\")\n",
    "                \n",
    "                # KEY FIX: Use the FULL text to find the code block\n",
    "                full_response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                \n",
    "                # DEBUG: Show what the model actually generated\n",
    "                new_text = full_response[len(full_prompt):]\n",
    "                print(f\"    Model response preview: {new_text[:200]}...\")\n",
    "                \n",
    "                code_match = re.search(r\"```python(.*?)```\", full_response, re.DOTALL)\n",
    "                if code_match:\n",
    "                    code = code_match.group(1).strip()\n",
    "                    print(f\"    Found code block: {len(code)} characters\")\n",
    "                    if code:\n",
    "                        # print(f\"    Code preview: {code[:100]}...\") # Optional debug\n",
    "                        output, status = self.run_python_code(code)\n",
    "                        \n",
    "                        if status == \"SUCCESS\":\n",
    "                            answer = self.extract_answer(output)\n",
    "                            if answer is not None and self.verify_solution(problem_text, code, answer):\n",
    "                                elapsed = time.time() - start_time\n",
    "                                print(f\"    ‚úÖ {topic.title()} success: {answer} ({elapsed:.1f}s)\")\n",
    "                                self.problem_cache[problem_hash] = answer\n",
    "                                return answer\n",
    "                            else:\n",
    "                                print(f\"    ‚ùå Code ran but no valid answer. Output: '{output}'\")\n",
    "                        else:\n",
    "                            print(f\"    ‚ùå {status}: {output}\")\n",
    "                else:\n",
    "                    # Fallback: If no closing tag, maybe it just stopped?\n",
    "                    if \"```python\" in prompt_content:\n",
    "                        print(f\"    ‚ö†Ô∏è No closing tag found. Trying raw text...\")\n",
    "                        output, status = self.run_python_code(new_text)\n",
    "                        if status == \"SUCCESS\":\n",
    "                             answer = self.extract_answer(output)\n",
    "                             if answer is not None and self.verify_solution(problem_text, new_text, answer):\n",
    "                                 return answer\n",
    "\n",
    "                    print(f\"    ‚ùå No valid code block found\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(\"    ‚ö†Ô∏è OOM - clearing cache\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"    ‚ùå Runtime error: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Generation error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚ùå {topic.upper()} failed after {elapsed:.1f}s\")\n",
    "        return 0\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION\n",
    "# ==========================================\n",
    "solver = SpecialistAIMO3Solver()\n",
    "\n",
    "# Add this quick test\n",
    "def test_basic_generation():\n",
    "    print(\"üß™ Testing basic code generation...\")\n",
    "    solver.load()\n",
    "    \n",
    "    if not solver.is_loaded:\n",
    "        print(\"‚ùå Skipping test because model failed to load.\")\n",
    "        return\n",
    "\n",
    "    # Use manual chat template in test too\n",
    "    test_prompt = \"<|user|>\\nWrite Python code to calculate 5 factorial and print the result.\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = solver.tokenizer(test_prompt, return_tensors=\"pt\").to(solver.model.device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = solver.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            pad_token_id=solver.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # FIX: Search the FULL response\n",
    "    full_response = solver.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Generated (Preview): {full_response[len(test_prompt):]}\")\n",
    "    \n",
    "    # Check if it contains valid code\n",
    "    code_match = re.search(r\"```python(.*?)```\", full_response, re.DOTALL)\n",
    "    if code_match:\n",
    "        code = code_match.group(1).strip()\n",
    "        print(f\"Extracted code: {code}\")\n",
    "        output, status = solver.run_python_code(code)\n",
    "        print(f\"Execution: {status}, Output: '{output}'\")\n",
    "    else:\n",
    "        # Fallback check\n",
    "        print(\"‚ö†Ô∏è Regex failed on full text. Trying loose match...\")\n",
    "        if \"import math\" in full_response:\n",
    "             # Manually grabbing code for test\n",
    "             code = full_response.split(\"```python\")[1].split(\"```\")[0]\n",
    "             output, status = solver.run_python_code(code)\n",
    "             print(f\"Execution (Fallback): {status}, Output: '{output}'\")\n",
    "        else:\n",
    "             print(\"‚ùå No code block generated\")\n",
    "\n",
    "def quick_integration_test():\n",
    "    \"\"\"Test the complete pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ TESTING COMPLETE PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"problem\": \"Calculate the number of positive integers less than 100 that are divisible by 3 or 5.\",\n",
    "            \"expected\": \"number_theory\",\n",
    "            \"hint\": \"Should use modular arithmetic\"\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"Find the area of a triangle with sides 5, 12, and 13.\",\n",
    "            \"expected\": \"geometry\", \n",
    "            \"hint\": \"Should use coordinate geometry\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test in enumerate(test_cases):\n",
    "        print(f\"\\nüß™ Test {i+1}: {test['problem'][:50]}...\")\n",
    "        \n",
    "        # Test classification\n",
    "        topic = solver.identify_topic(test['problem'])\n",
    "        print(f\"    Classification: {topic} (expected: {test['expected']})\")\n",
    "        \n",
    "        # Test prompt generation\n",
    "        prompt = solver.get_specialist_prompt(test['problem'], topic)\n",
    "        print(f\"    Prompt length: {len(prompt)} chars\")\n",
    "        print(f\"    Hint: {test['hint']}\")\n",
    "        \n",
    "        # Quick generation test (first 100 chars of response)\n",
    "        try:\n",
    "            inputs = solver.tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(solver.model.device)\n",
    "            with torch.no_grad():\n",
    "                generated_ids = solver.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,  # Just get a preview\n",
    "                    do_sample=False\n",
    "                )\n",
    "            preview = solver.tokenizer.decode(generated_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            print(f\"    Generation preview: {preview[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Generation failed: {e}\")\n",
    "\n",
    "def test_reference_problems():\n",
    "    \"\"\"Test the specialist router on actual AIMO problems\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ TESTING SPECIALIST ROUTER ON REFERENCE PROBLEMS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        possible_paths = [\n",
    "            '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv',\n",
    "            'reference.csv'\n",
    "        ]\n",
    "        data_path = next((p for p in possible_paths if os.path.exists(p)), None)\n",
    "        \n",
    "        if not data_path:\n",
    "            print(\"‚ö†Ô∏è Reference CSV not found.\")\n",
    "            return 0\n",
    "\n",
    "        reference_df = pd.read_csv(data_path)\n",
    "        print(f\"Testing on {len(reference_df)} reference problems...\")\n",
    "        \n",
    "        # Test first 2 problems to start\n",
    "        for i in range(min(2, len(reference_df))):\n",
    "            row = reference_df.iloc[i]\n",
    "            # Safe column access\n",
    "            prob = row.get('problem', row.get(reference_df.columns[1]))\n",
    "            ans = row.get('answer', row.get(reference_df.columns[-1]))\n",
    "            \n",
    "            try: ans = int(float(ans))\n",
    "            except: pass\n",
    "            \n",
    "            print(f\"\\nüî¢ REFERENCE PROBLEM {i+1}/2\")\n",
    "            print(f\"   Expected answer: {ans}\")\n",
    "            print(f\"   Problem preview: {prob[:80]}...\")\n",
    "            \n",
    "            result = solver.solve(prob)\n",
    "            \n",
    "            status = \"‚úÖ CORRECT\" if result == ans else \"‚ùå WRONG\"\n",
    "            print(f\"   {status} | Got: {result}\")\n",
    "            \n",
    "            # Clear memory between problems\n",
    "            torch.cuda.empty_cache()\n",
    "            time.sleep(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing reference problems: {e}\")\n",
    "\n",
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n",
    "    id_val = id_.item(0)\n",
    "    problem_text = problem.item(0)\n",
    "    try:\n",
    "        answer = solver.solve(problem_text)\n",
    "    except:\n",
    "        answer = 0\n",
    "    return pl.DataFrame({'id': [id_val], 'answer': [answer]})\n",
    "\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"üöÄ Starting Production Server...\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print(\"üî¨ Local Test Mode...\")\n",
    "    # Run basic test first\n",
    "    test_basic_generation()\n",
    "    \n",
    "    # If basic test works, test integration\n",
    "    if solver.is_loaded:\n",
    "        quick_integration_test()\n",
    "        \n",
    "        # Uncomment to test reference problems after integration test passes\n",
    "        test_reference_problems()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 247048,
     "modelInstanceId": 225305,
     "sourceId": 263434,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.430035,
   "end_time": "2025-11-23T06:53:08.603081",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-23T06:52:37.173046",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
